{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) IMPORT PACKAGES\n",
    "\n",
    "import base64\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pystac\n",
    "import requests\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import warnings\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "from distutils.command import sdist\n",
    "from folium import plugins\n",
    "from matplotlib import pyplot as plt\n",
    "from pyproj.crs import CRS\n",
    "from pystac_client import Client\n",
    "from pprint import pprint\n",
    "from shapely.geometry import box, mapping, Point, Polygon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Project specific packages\n",
    "from FH_Hydrosat import FH_StackedDataset\n",
    "from FH_Hydrosat import FH_Hydrosat\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) NOTEBOOK FUNCTIONS\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def stac_tile_search(collection, geom, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Log into STAC and search for a specified image collection.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    collection: list\n",
    "        List of tiles found in STAC.\n",
    "\n",
    "    geom: dict\n",
    "        A dictionary of coordinates defining point location to search.\n",
    "\n",
    "    start_date, end_date: str\n",
    "        Dates to search between.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    found_items: list\n",
    "        List of images found.\n",
    "    num_tiles: int\n",
    "        Num of images found.\n",
    "    itemjson: json\n",
    "        Json describing what search found.\n",
    "    \"\"\"\n",
    "    search = catalog.search(\n",
    "        collections=collection,\n",
    "        intersects=geom,\n",
    "        datetime=[start_date, end_date],\n",
    "        max_items=1000,\n",
    "    )\n",
    "\n",
    "    #found_items = list(search.items()) # for pystac-client >= 0.4.0\n",
    "    found_items = list(search.get_all_items())  # for pystac-client < 0.4.0\n",
    "\n",
    "    # Need this to get the url path for downloading the files to local machine.\n",
    "    itemjson = search.item_collection_as_dict() \n",
    "\n",
    "    # Filter out only the newest version of MODIS.\n",
    "    if collection == 'prepped_inputs_mcd43a4':\n",
    "        version_str = '.061_'\n",
    "        new_list = [i for i in found_items if version_str in i.id]\n",
    "        found_items = new_list\n",
    "\n",
    "    found_items.reverse()  # make the results ascending in time\n",
    "\n",
    "    num_tiles = len(found_items)\n",
    "    print(\"Colllection: {}.  {} Images found.\".format(collection, num_tiles))\n",
    "\n",
    "    return (found_items, num_tiles, itemjson)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def create_aoi_image_stack(asset, items, num_tiles, poly_gdf):\n",
    "    '''\n",
    "    Gets images, stacks them and sorts them by date and clips them down to a smaller\n",
    "    AOI size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    itmes: list \n",
    "        List of available images.\n",
    "    num_tiles: int\n",
    "        Number of tiles to download (days)\n",
    "    asset: str\n",
    "        Name of asset to get.\n",
    "    poly_gdf: geodataframe\n",
    "        A polygon to which the dataarray image will be clipped.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aoi_stack_ds: dataset as FH_StackedDataset object.\n",
    "        Stack of images clipped to AOI.\n",
    "\n",
    "    '''\n",
    "    images = FH_Hydrosat(items[:num_tiles], asset=asset)\n",
    "\n",
    "    # Stacks all the files into a dataset and then return a FH_StackedDataset object.\n",
    "    stacked_images = images.stack()\n",
    "    # Sort the dataset by time.\n",
    "    ds = stacked_images.ds.sortby('time')\n",
    "    \n",
    "    # Clip the big image into a smaller image using the poly_gdf AOI\n",
    "    # we defined in the Analysis Setup code cell.\n",
    "    clipped = FH_StackedDataset(ds.rio.clip(poly_gdf.geometry, all_touched=True, drop=True))\n",
    "    aoi_stack_ds = clipped.ds\n",
    "\n",
    "    return (aoi_stack_ds)\n",
    "\n",
    "# ---------------------------------------------------------------------------------   \n",
    "def extract_time_series(items, asset, bbox, tol, pad, band, var_name):\n",
    "    '''\n",
    "    Uses FH_Hydrosat class method point_time_series_from_items()\n",
    "    to extract only a time-series.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items: list\n",
    "        Image items returned from STAC search.\n",
    "    bbox: \n",
    "        Bounding box of coordinates for seacrh site.\n",
    "    tol: int\n",
    "        A search parameter in meters for finding point data.\n",
    "    var_name: str\n",
    "        Dataframe column name for data extracted.\n",
    "    asset: str\n",
    "        Search parameter for type of asset to be searched.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lst_df: dataframe\n",
    "        Dataframe containing date time series.\n",
    "    '''\n",
    "    # Sample the LST items.\n",
    "    lst_res = FH_Hydrosat(items, asset=asset)\n",
    "\n",
    "    # Set the point for time-series extraction.\n",
    "    point_wgs84 = Point(box(*bbox).centroid.x, box(*bbox).centroid.y)\n",
    "    \n",
    "    # Extract time-series data using function.\n",
    "    band = int(band) # band needs to be an int because it comes in as a string.\n",
    "    lst_k  = lst_res.point_time_series_from_items(point_wgs84, tol=tol, nproc=6, band=band) \n",
    "\n",
    "    # Create a datetime dataframe\n",
    "    lst_dt = lst_res.datetime\n",
    "    lst_df = pd.DataFrame({var_name: lst_k,\n",
    "                       'datetime': pd.to_datetime(lst_dt)}).sort_values(by='datetime')\n",
    "    \n",
    "    # Get the date in the correct/consistent format.\n",
    "    lst_df['date'] = [t.to_pydatetime().strftime('%Y-%m-%d') for t in lst_df['datetime']]\n",
    "    lst_df['date'] = pd.to_datetime(lst_df['date'])\n",
    "    lst_df.drop(columns='datetime', inplace=True)\n",
    "    lst_df.set_index('date', drop=True, inplace=True)\n",
    "    \n",
    "    return (lst_df)\n",
    "\n",
    "# --------------------------------------------------------------------------------- \n",
    "def read_ameriflux(data_path):\n",
    "    '''\n",
    "    Reads, extracts & processes meteorological csv file and returns data in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path: str\n",
    "        Path to the met datafile\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    df: dataframe\n",
    "        Dataframe containing met time series.\n",
    "    '''\n",
    "   \n",
    "    print('Reading file {}'.format(data_path))\n",
    "\n",
    "    df = pd.read_csv(data_path, header=0, na_values=[-9999.000000])\n",
    "\n",
    "    # Save value column names\n",
    "    value_cols = df.columns[2:]\n",
    "\n",
    "    # Convert timestamp objects\n",
    "    df['start'] = df['TIMESTAMP_START'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M.0\")\n",
    "    )\n",
    "    df['end'] = df['TIMESTAMP_END'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M.0\")\n",
    "    )\n",
    "\n",
    "    # Convert obs to UTC time.txt.\n",
    "    # # UTC_OFFSET is a global var.\n",
    "    df['start'] = df['start'] + timedelta(hours=UTC_OFFSET)\n",
    "    df['end'] = df['end'] + timedelta(hours=UTC_OFFSET)\n",
    "    df['start'] = df['start'].dt.tz_localize('UTC')\n",
    "    df['end'] = df['end'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Drop NA\n",
    "    df = df.dropna(subset=value_cols, how='all')\n",
    "\n",
    "    df = df.set_index('start')\n",
    "    col_order = (['end', 'TIMESTAMP_START', 'TIMESTAMP_END']\n",
    "                 + value_cols.to_list())\n",
    "    df = df[col_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def write_df_files(df, text):\n",
    "    \"\"\" \n",
    "    Writes a dataframe to a csv file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: dataframe\n",
    "        Dataframe to write to file.\n",
    "    text: str\n",
    "        String to add to output file name.\n",
    "    Returns:\n",
    "    -------\n",
    "    file_path: str\n",
    "        Path to the file so it can be opened later.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = \"{}_{}.csv\".format(analysis, text)\n",
    "    file_path =os.path.join(csv_path, file_name)\n",
    "    df.to_csv(path_or_buf=file_path, sep=',', na_rep='NA')\n",
    "\n",
    "    return(file_path)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "def read_df_files(file_path):\n",
    "    \"\"\" \n",
    "    Reads a csv file of a df back into a df. \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to file to open.\n",
    "    Returns:\n",
    "    -------\n",
    "    df: dataframe\n",
    "        Dataframe of data.\n",
    "    \"\"\"\n",
    "\n",
    "    #file_name = \"{}_{}.csv\".format(analysis, text)\n",
    "    #file_path =os.path.join(csv_path, file_name)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.time = pd.to_datetime(df.time, format='mixed')\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are succesfully logged in to the STAC server and you can now begin STAC queries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) STAC LOGIN\n",
    "\n",
    "# Open credentials file.\n",
    "with open('../secrets/creds.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "# Endecode the `username:password` combination\n",
    "# and use it to authorize access to the STAC API given by the `cat_url`\n",
    "# endpoint.userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "b64 = base64.b64encode(userpass.encode()).decode()\n",
    "headers = {'Authorization': 'Basic ' + b64}\n",
    "\n",
    "cat_url = 'https://fusion-stac.hydrosat.com'\n",
    "catalog = Client.open(cat_url, headers)\n",
    "\n",
    "if not catalog.id == 'stac-server':\n",
    "    print(\"You have failed to log into the STAC server\\n\")\n",
    "else:\n",
    "    print(\"You are succesfully logged in to the STAC server and you can now begin STAC queries.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POINT (-121.5223 38.1106)\n",
      "POINT (-121.5223 38.1106)\n",
      "Analysis setup comlete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) ANALYSIS SETUP.  AOI CLIPPING POLYGON CREATED.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# USER EDITS REQUIRED -------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Corn field Number.\n",
    "field_num = 4\n",
    "analysis = \"Corn{}\".format(field_num)\n",
    "\n",
    "# Want met obs in UTC time to match satellites.\n",
    "UTC_OFFSET = 7\n",
    "\n",
    "# Set dates for two growing seasons.\n",
    "start1 = '2022-06-01'\n",
    "end1 = '2022-09-30'\n",
    "\n",
    "# Insitu point locations\n",
    "insitu_met_points = {\n",
    "    'Corn1': [38.1094, -121.5361],\n",
    "    'Corn2': [38.1094, -121.5361],\n",
    "    'Corn3': [38.1094, -121.5361],\n",
    "    'Corn4': [38.1094, -121.5361],\n",
    "}\n",
    "\n",
    "# Center point of crop fields.\n",
    "crop_center_points = {\n",
    "    'Corn1': [-121.536114, 38.109538],\n",
    "    'Corn2': [-121.533, 38.10607],\n",
    "    'Corn3': [-121.52588, 38.10793],\n",
    "    'Corn4': [-121.52098, 38.10887], \n",
    "}\n",
    "\n",
    "# Coords for point extraction.\n",
    "# I.e. For Low NDVI pixels in this analysis.\n",
    "point_extraction_coords = {\n",
    "    'Corn1': [-121.535231, 38.108937],\n",
    "    'Corn2': [-121.533986, 38.105794],\n",
    "    'Corn3': [-121.525239, 38.110358],\n",
    "    'Corn4': [-121.5223, 38.11060], \n",
    "}\n",
    "\n",
    "aoi_coordinates = {\n",
    "    'Corn1': [\n",
    "        (-121.5352703314288,38.11067012116794),\n",
    "        (-121.537017491703,38.1103395663037),\n",
    "        (-121.5370233431901,38.10841814871685),\n",
    "        (-121.5352318761426,38.10893740470007),\n",
    "        (-121.5352703314288,38.11067012116794),\n",
    "   ],\n",
    "    'Corn2': [\n",
    "        (-121.5344444486311,38.10287555732225),\n",
    "        (-121.5314483994475,38.10375752472796),\n",
    "        (-121.5315333645822,38.10922251872468),\n",
    "        (-121.5344774233088,38.10830220203034),\n",
    "        (-121.5344444486311,38.10287555732225),\n",
    "    ],\n",
    "    'Corn3': [\n",
    "        (-121.5272352009598,38.10451747166002),\n",
    "        (-121.5244507592863,38.10532159430903),\n",
    "        (-121.5245771824385,38.1114312430658),\n",
    "        (-121.5273158126795,38.1105671020121),\n",
    "        (-121.5272352009598,38.10451747166002),\n",
    "    ],\n",
    "    'Corn4': [\n",
    "        (-121.523794568772,38.10500378161704),\n",
    "        (-121.5178846217463,38.10628347407603),\n",
    "        (-121.5178930837967,38.11294533147539),\n",
    "        (-121.5239298740425,38.1113683129804),\n",
    "        (-121.523794568772,38.10500378161704),\n",
    "    ],\n",
    "}\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Define path to met data\n",
    "data_path = os.path.join('../data', 'Ameriflux')\n",
    "\n",
    "# Define path to local/temp output data file.\n",
    "csv_path = os.path.join('./out')\n",
    "if not os.path.exists(csv_path):\n",
    "    os.mkdir('./out')\n",
    "    \n",
    "if analysis == 'Corn1':\n",
    "    \n",
    "    bbox = [\n",
    "        aoi_coordinates[analysis][0][0], aoi_coordinates[analysis][0][1], \n",
    "        aoi_coordinates[analysis][3][0], aoi_coordinates[analysis][3][1]\n",
    "        ]\n",
    "\n",
    "    # Manually defined point location of the pixel showing lower ndvi (stress)\n",
    "    p_geom = Point(point_extraction_coords[analysis])\n",
    "\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    \n",
    "    aoi = aoi_coordinates['Corn1']\n",
    "    poly = Polygon(aoi)\n",
    "    poly_gdf = gpd.GeoDataFrame({'geometry': [poly]}, crs=CRS.from_epsg(4326))\n",
    "    poly_gdf = poly_gdf.to_crs(crs=CRS.from_epsg(32610))\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "        \n",
    "elif analysis == 'Corn2':\n",
    "\n",
    "    bbox = [\n",
    "        aoi_coordinates[analysis][0][0], aoi_coordinates[analysis][0][1], \n",
    "        aoi_coordinates[analysis][3][0], aoi_coordinates[analysis][3][1]\n",
    "        ]\n",
    "\n",
    "    # Manually defined point location of the pixel showing lower ndvi (stress)\n",
    "    p_geom = Point(point_extraction_coords[analysis])\n",
    "\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    \n",
    "    aoi = aoi_coordinates['Corn2']\n",
    "    poly = Polygon(aoi)\n",
    "    poly_gdf = gpd.GeoDataFrame({'geometry': [poly]}, crs=CRS.from_epsg(4326))\n",
    "    poly_gdf = poly_gdf.to_crs(crs=CRS.from_epsg(32610))\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "\n",
    "elif analysis == 'Corn3':\n",
    "\n",
    "    bbox = [\n",
    "        aoi_coordinates[analysis][0][0], aoi_coordinates[analysis][0][1], \n",
    "        aoi_coordinates[analysis][3][0], aoi_coordinates[analysis][3][1]\n",
    "        ]\n",
    "    \n",
    "    # Manually defined point location of the pixel showing lower ndvi (stress)\n",
    "    p_geom = Point(point_extraction_coords[analysis])\n",
    "\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    \n",
    "    aoi = aoi_coordinates['Corn3']\n",
    "    poly = Polygon(aoi)\n",
    "    poly_gdf = gpd.GeoDataFrame({'geometry': [poly]}, crs=CRS.from_epsg(4326))\n",
    "    poly_gdf = poly_gdf.to_crs(crs=CRS.from_epsg(32610))\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "\n",
    "elif analysis == 'Corn4':\n",
    "\n",
    "    bbox = [\n",
    "        aoi_coordinates[analysis][0][0], aoi_coordinates[analysis][0][1], \n",
    "        aoi_coordinates[analysis][3][0], aoi_coordinates[analysis][3][1]\n",
    "        ]\n",
    "    \n",
    "    # Manually defined point location of the pixel showing lower ndvi (stress)\n",
    "    p_geom = Point(point_extraction_coords[analysis])\n",
    "\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    \n",
    "    aoi = aoi_coordinates['Corn4']\n",
    "    poly = Polygon(aoi)\n",
    "    poly_gdf = gpd.GeoDataFrame({'geometry': [poly]}, crs=CRS.from_epsg(4326))\n",
    "    poly_gdf = poly_gdf.to_crs(crs=CRS.from_epsg(32610))\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "                \n",
    "# Create dict of coords.  Will be used to create a polygon for our AOI.\n",
    "geom = {'type': 'Point', 'coordinates': crop_center_points[analysis]}\n",
    "\n",
    "print(\"Analysis setup comlete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5) FOLIUM MAP OF AOI.\n",
    "\n",
    "p_geom = Point(geom['coordinates'][0], geom['coordinates'][1])\n",
    "\n",
    "m = folium.Map(location=[p_geom.y, p_geom.x], zoom_start=15,\n",
    "               tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# Use satellite image for base map.\n",
    "tile = folium.TileLayer(\n",
    "    tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "    attr='Esri',\n",
    "    name='Esri Satellite',\n",
    "    overlay=False,\n",
    "    control=True\n",
    ").add_to(m)\n",
    "# ---------------------------------\n",
    "\n",
    "# add the polygon and centroid\n",
    "df = poly_gdf.to_crs(epsg=4326)\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    # Without simplifying the representation of each borough,\n",
    "    # the map might not be displayed\n",
    "    sim_geo = gpd.GeoSeries(r[\"geometry\"]).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"fillColor\": \"orange\"})\n",
    "    geo_j.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colllection: prepped_inputs_s2.  25 Images found.\n",
      "Colllection: prepped_inputs_mcd43a4.  36 Images found.\n",
      "First S2 date: <Item id=S2_10SFH_median_composite_20220606>\n",
      "Last S2 date: <Item id=S2_10SFH_median_composite_20220924>\n",
      "First MCD date: <Item id=MCD43A4.061_20220822_10SFH>\n",
      "Last MCD date: <Item id=MCD43A4.061_20220930_10SFH>\n"
     ]
    }
   ],
   "source": [
    "# 6) SENTINEL & MODIS IMAGE SEARCH\n",
    "\n",
    "collections = [\"prepped_inputs_s2\"]\n",
    "\n",
    "# Search STAC for available images.\n",
    "(s2_items, num_s2_tiles, itemjson) = stac_tile_search(\"prepped_inputs_s2\", geom, start1, end1)\n",
    "(mcd43a4_items, num_mcd43a4_tiles, itemjson) = stac_tile_search(\"prepped_inputs_mcd43a4\", geom, start1, end1)\n",
    "\n",
    "# Print out the first and last dates of the found items (images).\n",
    "print(f\"First S2 date: {s2_items[0]}\")\n",
    "print(f\"Last S2 date: {s2_items[-1]}\")\n",
    "\n",
    "print(f\"First MCD date: {mcd43a4_items[0]}\")\n",
    "print(f\"Last MCD date: {mcd43a4_items[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#* 7) EXTRACT SENTINEL2 INTO DATAARRAYS & COMPUTE NDVI\n",
    "\n",
    "# Sentinel 2 --------------------------------------------------------\n",
    "\n",
    "# Get images from stack.\n",
    "(aoi_stack_ds) = create_aoi_image_stack(\n",
    "    'surface_reflectance', s2_items, num_s2_tiles, poly_gdf)\n",
    "\n",
    "# Pull out NIR and Red bands.\n",
    "red_aoi_ds = aoi_stack_ds.isel(band=2)\n",
    "nir_aoi_ds = aoi_stack_ds.isel(band=6)\n",
    "\n",
    "# Compute NDVI.\n",
    "ndvi_s2_aoi_ds = (nir_aoi_ds - red_aoi_ds) / (nir_aoi_ds + red_aoi_ds)\n",
    "\n",
    "# Modis --------------------------------------------------------------\n",
    "\n",
    "# Get images from stack\n",
    "(aoi_stack_ds) = create_aoi_image_stack(\n",
    "    'surface_reflectance', mcd43a4_items, num_mcd43a4_tiles, poly_gdf)\n",
    "\n",
    "# Pull out NIR and Red bands.\n",
    "red_aoi_mcd_ds = aoi_stack_ds.isel(band=0)\n",
    "nir_aoi_mcd_ds = aoi_stack_ds.isel(band=1)\n",
    "\n",
    "# Compute NDVI.\n",
    "ndvi_aoi_mcd_ds = (nir_aoi_mcd_ds - red_aoi_mcd_ds) / (nir_aoi_mcd_ds + red_aoi_mcd_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8) PLOT SENTINEL 2 DATARRAY IMAGES OF NDVI MEAN & VARIANCE\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,5))\n",
    "\n",
    "# Convert our slice time endpoints to compatabile format.\n",
    "d1 = pd.to_datetime('2022-07-01', utc=True)\n",
    "d2 = pd.to_datetime('2022-07-31', utc=True)\n",
    "\n",
    "# The point location of the pixel showing lower ndvi (stress).\n",
    "point_df = gpd.GeoDataFrame({'geometry':[p_geom]}, crs=CRS.from_epsg(4326))\n",
    "raster_crs = CRS.from_wkt(ndvi_s2_aoi_ds.spatial_ref.crs_wkt)\n",
    "buffer_dist = 20 # 1km in local UTM zone\n",
    "point_poly_df = point_df.to_crs(raster_crs).buffer(buffer_dist, cap_style = 3) # square buffer\n",
    "centroid = point_poly_df.geometry[0].centroid\n",
    "point_x, point_y = (centroid.x, centroid.y)\n",
    "\n",
    "# Select slices, compute stats, and plot.\n",
    "# Mean and Variance.\n",
    "a = ndvi_s2_aoi_ds.sel(time=slice(d1,d2)).mean(dim='time').plot(ax=ax1)\n",
    "b = ndvi_s2_aoi_ds.sel(time=slice(d1,d2)).var(dim='time').plot(ax=ax2)\n",
    "\n",
    "# Adjust plot elements.\n",
    "\n",
    "ax1.set_title('Mean of Sentinel 2 NDVI')\n",
    "ax2.set_title('Variance of Sentinel 2 NDVI')\n",
    "title = \"Sentinel 2 Mean NDVI July 1-31, 2022\\nCorn Field {}\".format(field_num)\n",
    "fig.suptitle(title, fontsize='12')\n",
    "\n",
    "# Plot a point where ndvi is lower.  Will later extract point data here.\n",
    "ax1.plot(point_x, point_y, markersize=15, marker=\".\", color=\"m\")\n",
    "ax2.plot(point_x, point_y, markersize=15, marker=\".\", color=\"m\")\n",
    "\n",
    "ax1.set_yticklabels('')\n",
    "ax2.set_yticklabels('')\n",
    "\n",
    "ax1.set_xticklabels('')\n",
    "ax2.set_xticklabels('')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "ax1.set_ylabel('')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) PLOT ONLY SENTINEL 2 DATARRAY IMAGE OF NDVI MEAN\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(4,6))\n",
    "\n",
    "# Convert our slice time endpoints to compatabile format.\n",
    "d1 = pd.to_datetime('2022-07-01', utc=True)\n",
    "d2 = pd.to_datetime('2022-07-31', utc=True)\n",
    "\n",
    "# The point location of the pixel showing lower ndvi (stress).\n",
    "point_df = gpd.GeoDataFrame({'geometry':[p_geom]}, crs=CRS.from_epsg(4326))\n",
    "raster_crs = CRS.from_wkt(ndvi_s2_aoi_ds.spatial_ref.crs_wkt)\n",
    "buffer_dist = 20 # 1km in local UTM zone\n",
    "point_poly_df = point_df.to_crs(raster_crs).buffer(buffer_dist, cap_style = 3) # square buffer\n",
    "centroid = point_poly_df.geometry[0].centroid\n",
    "point_x, point_y = (centroid.x, centroid.y)\n",
    "\n",
    "# Select slices, compute stats, and plot.\n",
    "# Mean and Variance.\n",
    "a = ndvi_s2_aoi_ds.sel(time=slice(d1,d2)).mean(dim='time').plot(ax=ax1)\n",
    "# Plot a point where ndvi is lower.  Will later extract point data here.\n",
    "ax1.plot(point_x, point_y, markersize=15, marker=\".\", color=\"m\")\n",
    "\n",
    "# Adjust plot elements.\n",
    "\n",
    "ax1.set_title('Mean of Sentinel 2 NDVI')\n",
    "title = \"Sentinel 2 Mean NDVI (July 1-31, 2022)\\nCorn Field {}\".format(field_num)\n",
    "fig.suptitle(title, fontsize='12')\n",
    "\n",
    "ax1.set_yticklabels('')\n",
    "ax2.set_yticklabels('')\n",
    "ax1.set_xticklabels('')\n",
    "ax2.set_xticklabels('')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9) CONVERT SENTINEL 2 TO DATAFRAME, RESAMPLE & WRITE TO FILE.\n",
    "\n",
    "# Resampling simplifies the timestamp in that it removes the hours and minutes \n",
    "# and makes each day's dt consistent.  In addition, resampling fills in missing dates \n",
    "# with NA which will help with plotting consistency.\n",
    "\n",
    "# Sentinel 2 ------------------------------------------------------\n",
    "# Convert dataaset to a dataframe.\n",
    "s2_ndvi_df = ndvi_s2_aoi_ds[0:24,:,:].to_dataframe(name='s2_ndvi')\n",
    "# Drop uneeded columns.\n",
    "s2_ndvi_df.drop(columns=['spatial_ref'], inplace=True)\n",
    "# Convert multilevel index to single level.\n",
    "s2_ndvi_df = s2_ndvi_df.reset_index(level=[1, 2])\n",
    "# Put index into dt64 format.\n",
    "s2_ndvi_df.index = pd.to_datetime(s2_ndvi_df.index)\n",
    "# Resample to just the y-m-d and take the mean() which in this case\n",
    "# will be the mean ndvi for the AOI.\n",
    "s2_ndvi_resampled_df = s2_ndvi_df.groupby(['x','y']).resample('D', origin=pd.to_datetime(start1, utc=s2_ndvi_df.index.tz)).mean()\n",
    "# s2_ndvi_mean_df = s2_ndvi_df.resample('D').mean()  # NExt cell instead.\n",
    "# Drop uneeded cols and make a single index instead of multi.\n",
    "s2_ndvi_resampled_df.drop(columns=['y','x'], inplace=True)\n",
    "s2_ndvi_resampled_df = s2_ndvi_resampled_df.reset_index(level=[0, 1])\n",
    "# Resort so that rows are chronologically grouped.\n",
    "s2_ndvi_resampled_sort_df = s2_ndvi_resampled_df.sort_index()\n",
    "\n",
    "# CREATE DATAFRAMES OF MEAN SENTINEL 2 AND MODIS NDVI.\n",
    "s2_ndvi_mean_df = s2_ndvi_resampled_df.groupby(['time']).resample('D', origin=pd.to_datetime(start1, utc=s2_ndvi_df.index.tz)).mean()\n",
    "s2_ndvi_mean_df.rename(columns={\"s2_ndvi\": \"s2_mean_ndvi\"}, inplace=True)\n",
    "s2_ndvi_mean_df = s2_ndvi_mean_df.reset_index(level=[1])\n",
    "s2_ndvi_mean_df.index = pd.to_datetime(s2_ndvi_mean_df.time)\n",
    "\n",
    "# Write dataframe to a file so that we dont have to \n",
    "# convert the dataarrays every time.\n",
    "#file_name = \"{}_s2_ndvi_aoi_csv\".format(analysis)\n",
    "#ndvi_csv_aoi_path =os.path.join(csv_path, file_name)\n",
    "#s2_ndvi_resampled_sort_df.to_csv(path_or_buf=ndvi_csv_aoi_path, sep=',', na_rep='NA')\n",
    "#\n",
    "#file_name = \"{}_s2_ndvi_mean_csv\".format(analysis)\n",
    "#ndvi_csv_mean_path =os.path.join(csv_path, file_name)\n",
    "#s2_ndvi_mean_df.to_csv(path_or_buf=ndvi_csv_mean_path, sep=',', na_rep='NA')\n",
    "\n",
    "# Write dataframes to csv files.\n",
    "ndvi_aoi_path = write_df_files(s2_ndvi_resampled_sort_df, 's2_ndvi_aoi')\n",
    "ndvi_mean_path = write_df_files(s2_ndvi_mean_df, 's2_ndvi_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11) CREATE AOI BOXPLOT OF SENTINEL2 NDVI AOI PIXELS.\n",
    "\n",
    "# READ CSV OUTPUT FILES INTO NDVI DF\n",
    "# Pass the file path returned by the write_df_files() function.\n",
    "file_name = \"{}_{}.csv\".format(analysis, 's2_ndvi_aoi')\n",
    "ndvi_aoi_path =os.path.join(csv_path, file_name)\n",
    "s2_ndvi_resampled_df = read_df_files(ndvi_aoi_path)\n",
    "\n",
    "file_name = \"{}_{}.csv\".format(analysis, 's2_ndvi_mean')\n",
    "ndvi_mean_path =os.path.join(csv_path, file_name)\n",
    "s2_ndvi_mean_df = read_df_files(ndvi_mean_path)\n",
    "\n",
    "# Get rid of the time/hours from the date.\n",
    "s2_ndvi_resampled_df.time = pd.to_datetime(s2_ndvi_resampled_df.time, format='mixed').dt.date\n",
    "# For some reason the time has to be converted again to get a dt object.\n",
    "s2_ndvi_resampled_df.time = pd.to_datetime(s2_ndvi_resampled_df.time)\n",
    "\n",
    "s2_ndvi_mean_df.time = pd.to_datetime(s2_ndvi_mean_df.time, format='mixed').dt.date\n",
    "# For some reason the time has to be converted again to get a dt object.\n",
    "s2_ndvi_mean_df.time = pd.to_datetime(s2_ndvi_mean_df.time)\n",
    "\n",
    "# Fill in missing start dates so we can start from the 1st of the month.\n",
    "dates = pd.date_range(start=start1, end=end1).to_frame()\n",
    "s2_ndvi_resampled_df = pd.merge(\n",
    "    s2_ndvi_resampled_df, dates, left_on='time', right_on=0,\n",
    "    how='outer').sort_values(by=['time']).drop(columns=0)\n",
    "\n",
    "# Create plot.\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "fig.suptitle('Sentinel 2 NDVI (June - September 2022)', fontsize='12')\n",
    "\n",
    "# Make box plots.\n",
    "title = \"Corn Field {}\".format(field_num)\n",
    "box = (\n",
    "    sns.boxplot(\n",
    "    x=s2_ndvi_resampled_df['time'].apply(lambda x: x.strftime('%Y-%m-%d')),\n",
    "    #x=s2_ndvi_resampled_df['time'].strftime('%Y-%m-%d'),\n",
    "    y=s2_ndvi_resampled_df['s2_ndvi'],\n",
    "    ax=ax1)\n",
    "    .set(title=title),\n",
    "    )\n",
    "\n",
    "ax1.set_ylabel('Sentinel 2 NDVI')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "ax1.xaxis.set_major_locator( mdates.DayLocator( bymonthday=1) )\n",
    "ax1.tick_params(labelrotation=25)\n",
    "\n",
    "ax1.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2:  Get meteorology & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ../data/Ameriflux/US-Bi2_HH_201704270000_202301010000.csv\n"
     ]
    }
   ],
   "source": [
    "#* 12) GET METEOROLOGY INTO DATAFRAME FOR CATD CALCS.\n",
    "\n",
    "# Will use met_df in next code cell.\n",
    "met_df = read_ameriflux(met_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Temperature.\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(9, 4))\n",
    "\n",
    "met_df[start1:end1].TA.resample('D').max().plot(ax=ax)\n",
    "fig.suptitle('Maximum Crop Field Air Temperature')\n",
    "ax.set_title('(Used for CATD Calculation)', fontsize=10)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Temperature (C)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Extract LST, Compute CATD, and Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#* 13) EXTRACT AOI LST DATARRAY.  COMPUTE CATD & CREAT CATD DATARRAYS.\n",
    "\n",
    "# Dictionary keys are: var_name, tolerance, resolution, plot color.\n",
    "#     var_name: used as column name in dataframe.\n",
    "#     tolerance (m): search parameter in meters for extracting point data.\n",
    "#     resolution (m): this is the legend when plotting\n",
    "#     plot color: for plotting\n",
    "\n",
    "# See https://hydrosat.github.io/fusion-hub-docs/3-FH-API-Spec.html\n",
    "# for asset specifications.\n",
    "\n",
    "# prepped_inputs_landsat = resampled to 30 m\n",
    "# prepped_inputs_mod21a1d = 1000 m\n",
    "# pydms_sharpened_modis = 500 m\n",
    "# pydms_sharpened_landsat = downscaled to 20 m\n",
    "# starfm_predictions_modis_landsat = 20 m\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# USER EDITS POSSIBLE -------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "asset_dict = {\n",
    "    \"prepped_inputs_mod21a1d\": ['mod21a1d_lst', 2000, 1000, 'brown'],\n",
    "    \"pydms_sharpened_modis\": ['pydms_modis_lst', 1000, 500, 'green'],\n",
    "    \"prepped_inputs_landsat\": ['lsat_lst', 60, 30, 'black'],\n",
    "    \"pydms_sharpened_landsat\": ['pydms_lsat_lst', 40, 20, 'blue'],\n",
    "    \"starfm_predictions_modis_landsat\": ['starfm_lst', 40, 20, 'red'],\n",
    "}\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "catd_list = [] # A list that will hold single timestep da's of catd.\n",
    "lst_da_list = []  # A list holding each collections LST dat.\n",
    "catd_da_list = []  #  Alist holding each collections catd dat.\n",
    "\n",
    "# Loop through the dictionary.\n",
    "for i, asset in enumerate(asset_dict):\n",
    "\n",
    "    # Just to make sure these lists get reset after each loop iteration.\n",
    "    found_items1 = []\n",
    "    found_items2 = []\n",
    "    found_items = []\n",
    "\n",
    "    (found_items, num_tiles, itemjson) = stac_tile_search(asset, geom, start1, end1)\n",
    "    \n",
    "    # Extract LST data from AOI. ----------------\n",
    "    (aoi_lst_da) = create_aoi_image_stack(\n",
    "            'lst', found_items, num_tiles, poly_gdf)\n",
    "\n",
    "    # Want to process each timesteps CATD so we iterate through\n",
    "    # time and work with a single dimension at at time.\n",
    "    for date, aoi in aoi_lst_da.groupby('time'):\n",
    "\n",
    "        # Get TA from dataframe using each time step (date).\n",
    "        ta = met_df.TA.iloc[met_df.index.get_indexer([date], method='nearest')]\n",
    "        # Convert from C to K.\n",
    "        ta = ta + 273.15\n",
    "        # Compute CATD.  Subtract the TA value from each AOI pixel.\n",
    "        catd = aoi - ta.values\n",
    "        # Append new CATD da to a list.\n",
    "        catd_list.append(catd)\n",
    "\n",
    "    # After each date has been computed.  Concatenate them all together.\n",
    "    catd_ds = xr.concat(catd_list, coords='minimal', dim='time', compat='override').squeeze()\n",
    "    # Done with the current CATD list so reset for next asset.\n",
    "    catd_list = []\n",
    "    \n",
    "    catd_da_list.append(catd_ds)\n",
    "    lst_da_list.append(aoi_lst_da.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 14) PLOT AOI DATAARRAY IMAGES OF CATD MEAN & VARIANCE\n",
    "\n",
    "fig, (ax2, ax3, ax4, ax5) = plt.subplots(1,4, figsize=(9,3))\n",
    "\n",
    "# Select slices, compute mean, and plot.\n",
    "\n",
    "# Noted catd_da_list[0] is 1000m Modis so only once cell is extracted for the AOI.  \n",
    "# This means there is one less dimension compared to the other assets.  You can \n",
    "# view the dimensions by printing out catd_da_list[0] vs catd_da_list[1].\n",
    "#a = catd_da_list[0][:,:].mean(dim='time').plot(ax=ax1)\n",
    "b = catd_da_list[1][:,:,:].mean(dim='time').plot(ax=ax2)\n",
    "c = catd_da_list[2][:,:,:].mean(dim='time').plot(ax=ax3)\n",
    "d = catd_da_list[3][:,:,:].mean(dim='time').plot(ax=ax4)\n",
    "e = catd_da_list[4][:,:,:].mean(dim='time').plot(ax=ax5)\n",
    "\n",
    "# Adjust plot elements.\n",
    "\n",
    "title = \"Mean CATD (June - September 2022)\\nCorn Field {}\".format(field_num)\n",
    "fig.suptitle(title)\n",
    "\n",
    "ax2.set_title('Sharpened Modis')\n",
    "ax3.set_title('Prepped Landsat')\n",
    "ax4.set_title('Sharpened Landsat')\n",
    "ax5.set_title('Fused Landsat & Modis')\n",
    "\n",
    "ax2.title.set_size(10)\n",
    "ax3.title.set_size(10)\n",
    "ax4.title.set_size(10)\n",
    "ax5.title.set_size(10)\n",
    "\n",
    "ax2.set_yticklabels('')\n",
    "ax3.set_yticklabels('')\n",
    "ax4.set_yticklabels('')\n",
    "ax5.set_yticklabels('')\n",
    "\n",
    "ax2.set_xticklabels('')\n",
    "ax3.set_xticklabels('')\n",
    "ax4.set_xticklabels('')\n",
    "ax5.set_xticklabels('')\n",
    "\n",
    "ax2.set_xlabel('')\n",
    "ax3.set_xlabel('')\n",
    "ax4.set_xlabel('')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "ax2.set_ylabel('')\n",
    "ax3.set_ylabel('')\n",
    "ax4.set_ylabel('')\n",
    "ax5.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) PLOT AOI DATAARRAY IMAGES OF CATD MEAN & VARIANCE\n",
    "\n",
    "fig, (ax5) = plt.subplots(1,1, figsize=(4,6))\n",
    "\n",
    "# Select slices, compute mean, and plot.\n",
    "# Convert our slice time endpoints to compatabile format.\n",
    "d1 = pd.to_datetime('2022-07-01')\n",
    "d2 = pd.to_datetime('2022-07-31')\n",
    "\n",
    "# Noted catd_da_list[0] is 1000m Modis so only once cell is extracted for the AOI.  \n",
    "# This means there is one less dimension compared to the other assets.  You can \n",
    "# view the dimensions by printing out catd_da_list[0] vs catd_da_list[1].\n",
    "#e = catd_da_list[4][:,:,:].mean(dim='time').plot(ax=ax5, label='CATD (Degrees C)')\n",
    "e = catd_da_list[4].sel(time=slice(d1,d2)).mean(dim='time').plot(ax=ax5, label='CATD (Degrees C)')\n",
    "ax5.plot(point_x, point_y, markersize=15, marker=\".\", color=\"m\")\n",
    "# Adjust plot elements.\n",
    "\n",
    "title = \"Mean CATD (July 1-31, 2022)\\nCorn Field {}\".format(field_num)\n",
    "fig.suptitle(title)\n",
    "\n",
    "ax5.set_title(\"Mean of Fused CATD ($^\\circ$C)\")\n",
    "ax5.set_yticklabels('')\n",
    "ax5.set_xticklabels('')\n",
    "ax5.set_xlabel('')\n",
    "ax5.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 15) CONVERT AOI CATD DATAARRAYS INTO DATAFRAMES & WRITE TO FILES.\n",
    "\n",
    "# Lists to hold datframes for each asset\n",
    "lst_df_list = []\n",
    "catd_df_list = []\n",
    "i=0\n",
    "\n",
    "# Loop through the dictionary.\n",
    "for i, asset in enumerate(asset_dict):\n",
    "\n",
    "    print (asset)\n",
    "\n",
    "    # Name the columns to hold catd.\n",
    "    catd_col_name = \"{}_{}_catd\".format(analysis, asset)\n",
    "    catd_df = catd_da_list[i].to_dataframe(name=catd_col_name)\n",
    "    \n",
    "    # Get rid of unneeded columns. --------\n",
    "    catd_df.reset_index(level=[1, 2], inplace=True)\n",
    "\n",
    "    # Resampling fills in dates that dont exist in the data.\n",
    "    catd_resampled_df = catd_df.groupby(['x','y']).resample('D').mean()\n",
    "    catd_resampled_df = catd_resampled_df.droplevel([0,1])\n",
    "\n",
    "    # Re-sort so that rows are chronologically grouped.\n",
    "    catd_resampled_df.sort_index(inplace=True)\n",
    "\n",
    "    # Write dfs to files.\n",
    "    text = \"{}_catd_aoi\".format(asset)\n",
    "    catd_aoi_path = write_df_files(catd_resampled_df, text)\n",
    "\n",
    "    # Create a list of dataframes.\n",
    "    catd_df_list.append(catd_resampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 16) AOI ANNUAL BOXPLOTS OF CATD\n",
    "\n",
    "# Create a figure.  Axes will be created in the loop.\n",
    "fig = plt.figure(figsize=(9, 12))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "fig.suptitle('CATD vs. Sentinel 2 NDVI', weight='bold', fontsize=20)\n",
    "\n",
    "# Create a counter for getting the number of axes correct.\n",
    "j = 0\n",
    "\n",
    "# Loop through each collection in the dictionary.\n",
    "for i, asset in enumerate(asset_dict):\n",
    "\n",
    "    # Get the dataframe out of the list but will skip the two MODIS\n",
    "    # dataframes because there is only mid-august and on data.\n",
    "    if i <= 1: continue\n",
    "\n",
    "    # READ CSV OUTPUT FILES INTO NDVI DF\n",
    "\n",
    "    file_name = \"{}_{}_catd_aoi.csv\".format(analysis, asset)\n",
    "    file_path =os.path.join(csv_path, file_name)\n",
    "\n",
    "    df_new_catd = read_df_files(file_path)\n",
    "    df_new_catd.index = df_new_catd['time']\n",
    "\n",
    "    # Make box plots. --------------------\n",
    "\n",
    "    # Create an axes dynamically.  \n",
    "    ax = plt.subplot(3, 1, j + 1)\n",
    "    # Increment the axes number up one after each valid dataset.\n",
    "    j = j + 1\n",
    "    # Create a twin axes for catd and ndvi.\n",
    "    twinax0 = ax.twinx()\n",
    "\n",
    "    # If statements catch any empty dataframes and doesnt try to plot them up.\n",
    "    if df_new_catd['2022-06-01':'2022-09-15'].size > 0:\n",
    "\n",
    "        catd_col_name = \"{}_{}_catd\".format(analysis, asset)\n",
    "\n",
    "        # Plot CATD.\n",
    "        (sns.boxplot(\n",
    "                x=df_new_catd['2022-06-04':'2022-09-15'].index.strftime('%Y-%m-%d'),\n",
    "                y=df_new_catd['2022-06-04':'2022-09-15'][catd_col_name],\n",
    "                ax=ax)\n",
    "        .set(title=\"{}\".format(asset))\n",
    "        )\n",
    "                \n",
    "        # Plot daily mean NDVI as points.\n",
    "        (sns.scatterplot(\n",
    "                x=s2_ndvi_mean_df['2022-06-04':'2022-09-15'].index.astype(str),\n",
    "                y=s2_ndvi_mean_df['2022-06-04':'2022-09-15']['s2_mean_ndvi'], \n",
    "                ax=twinax0, color='green')\n",
    "        )\n",
    "                \n",
    "        # Plot daily mean NDVI as a dashed line in addition to the points.\n",
    "        (sns.lineplot(\n",
    "                x=s2_ndvi_mean_df['2022-06-04':'2022-09-15'].index.astype(str),\n",
    "                y=s2_ndvi_mean_df['2022-06-04':'2022-09-15']['s2_mean_ndvi'], \n",
    "                ax=twinax0, color='green', linestyle='dashed')\n",
    "        )\n",
    "\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\n",
    "    ax.set_ylabel('Fused CATD (K)')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylim(-40,40)\n",
    "    \n",
    "    twinax0.set_ylabel(ylabel='Sentinel 2 Mean AOI NDVI', color='green')\n",
    "    twinax0.set_ylim(0,1)\n",
    "    twinax0.tick_params(color='green')\n",
    "\n",
    "    ax.grid()\n",
    "\n",
    "    # Here we want to highlight an area of data on the sharoened landsat (i==3)\n",
    "    # and the fused (i==4).\n",
    "    if i == 3:\n",
    "        plt.axvspan(56, 58, ymin=.12, ymax=.52, alpha=.3)\n",
    "    if i == 4:\n",
    "        plt.axvspan(56, 66, ymin=.10, ymax=.50, alpha=.3)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 17) PLOT AOI Rolling CATD AND NDVI.\n",
    "\n",
    "# Here we take the starfm fused product, clean it of outliers, and run a rolling mean on it.\n",
    "# We'll do the same with Sentinel NDVI and plot them together.\n",
    "\n",
    "# Open up the dataframe csv file.\n",
    "file_name = \"{}_{}_catd_aoi.csv\".format(analysis, \"starfm_predictions_modis_landsat\")\n",
    "file_path =os.path.join(csv_path, file_name)\n",
    "df_new_catd = read_df_files(file_path)\n",
    "df_new_catd.index = df_new_catd['time']\n",
    "\n",
    "# Get the dataframe out of the list that we are storing it in.\n",
    "starfm_df = df_new_catd[start1:end1]\n",
    "\n",
    "# Clean the catd df.  Will take only positive CATD values.  That may not be entirely\n",
    "# fair, but will do it in this case.\n",
    "key = \"{}_starfm_predictions_modis_landsat_catd\".format(analysis)\n",
    "starfm_clean_df = starfm_df[\n",
    "    (starfm_df[key] >= 0) & \n",
    "    (starfm_df[key] < 25)]\n",
    "\n",
    "# Create a dual axis figure.\n",
    "fig, (ax1) = plt.subplots(1,1,figsize=(8, 4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot rolling median\n",
    "starfm_clean_df[key].rolling(window='7D', min_periods=3).mean().plot(ax=ax1, label='7-Day CATD Rolling Mean')\n",
    "\n",
    "# Plot NDVI.  First just the points then a dashdot line for visualization.\n",
    "s2_ndvi_mean_df.index = pd.to_datetime(s2_ndvi_mean_df.time)\n",
    "s2_ndvi_mean_df[start1:end1].plot.scatter(x='time', y='s2_mean_ndvi',color='green', ax=ax2, label='S2 Mean NDVI')\n",
    "\n",
    "(sns.lineplot(\n",
    "            x=s2_ndvi_mean_df[start1:end1].index,\n",
    "            y=s2_ndvi_mean_df[start1:end1]['s2_mean_ndvi'], \n",
    "            ax=ax2, color='green', linestyle='dashdot')\n",
    ")\n",
    "\n",
    "# Hard code in vertical shading denoting the .7 NDVI \"full vegetation\" point.\n",
    "plt.axvspan('2022-06-26', '2022-08-18', alpha=.1)\n",
    "# Add line reflecting an ndvi of .7 which we assume is near a fully vegetated state.\n",
    "ax2.axhline(y=.7, color='green')\n",
    "\n",
    "fig.suptitle('7-Day Rolling Mean Fused CATD & Mean Sentinel 2 NDVI')\n",
    "plt.title(\"Corn Field {}\".format(field_num))\n",
    "# (CATD data is cleaned to include only CATD between 0 and 25.)')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('CATD ($^\\circ$C)', color='blue')\n",
    "ax1.tick_params(axis='y', colors='blue')\n",
    "\n",
    "ax2.set_ylabel('NDVI: Sentinel 2 Daily Mean', color='green')\n",
    "ax2.tick_params(axis='y', colors='green')\n",
    "\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1, 0.95))\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1, .78))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4:  Extract & Plot data from \"stressed\" point location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#* EXTRACT CATD & NDVI POINT DATA FROM DATAARRAYS, CONVERT TO DATAFRAMES, & WRITE FILES.\n",
    "\n",
    "# Get the starfm da out of our list from before.\n",
    "catd_starfm_aoi_da = catd_da_list[4]\n",
    "\n",
    "# The point location of the pixel showing lower ndvi (stress).\n",
    "point_df = gpd.GeoDataFrame({'geometry':[p_geom]}, crs=CRS.from_epsg(4326))\n",
    "raster_crs = CRS.from_wkt(ndvi_s2_aoi_ds.spatial_ref.crs_wkt)\n",
    "buffer_dist = 20 # 1km in local UTM zone\n",
    "point_poly_df = point_df.to_crs(raster_crs).buffer(buffer_dist, cap_style = 3) # square buffer\n",
    "centroid = point_poly_df.geometry[0].centroid\n",
    "point_x, point_y = (centroid.x, centroid.y)\n",
    "\n",
    "# Select pixel data and convert to dataframe.\n",
    "ndvi_s2_pnt_df = ndvi_s2_aoi_ds.sel(x=point_x, y=point_y, method='nearest', tolerance=40).to_dataframe('pnt_ndvi')\n",
    "catd_starfm_pnt_df = catd_starfm_aoi_da.sel(x=point_x, y=point_y, method='nearest', tolerance=40).to_dataframe('pnt_catd')\n",
    "\n",
    "# Write dfs to files.\n",
    "ndvi_point_path = write_df_files(ndvi_s2_pnt_df, 'ndvi_point')\n",
    "catd_point_path = write_df_files(catd_starfm_pnt_df, 'catd_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) PLOT USING DATAFRAMES POINT STRESS PIXEL CATD & NDVI.\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1,figsize=(8, 4))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Read df csv files into dfs.\n",
    "file_name = \"{}_{}.csv\".format(analysis, 'ndvi_point')\n",
    "ndvi_point_path =os.path.join(csv_path, file_name)\n",
    "\n",
    "file_name = \"{}_{}.csv\".format(analysis, 'catd_point')\n",
    "catd_point_path =os.path.join(csv_path, file_name)\n",
    "\n",
    "ndvi_s2_pnt_df = read_df_files(ndvi_point_path)\n",
    "catd_starfm_pnt_df = read_df_files(catd_point_path)\n",
    "\n",
    "ndvi_s2_pnt_df.plot(x='time', y='pnt_ndvi', color='green', linestyle='dashdot', marker='o', ax=ax2, label='Sentinel 2 NDVI')\n",
    "catd_starfm_pnt_df.plot(x='time', y='pnt_catd',color='blue', linestyle='dashdot', marker='o', ax=ax1, label='STARFM CATD')\n",
    "\n",
    "fig.suptitle('Fused CATD and Sentinel 2 NDVI')\n",
    "plt.title(\"Corn Field {}\".format(field_num))\n",
    "\n",
    "ax1.set_ylabel('Fused CATD (K)', color='blue')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylim(0,40)\n",
    "ax1.tick_params(axis='y', colors='blue')\n",
    "    \n",
    "ax2.set_ylabel(ylabel='Sentinel 2 NDVI', color='green')\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.tick_params(axis='y', colors='green')\n",
    "\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1, 0.95))\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1, .78))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f8cb41e19b5dcf5f26e7d3640eb2927eaa0095f403f6e610552a8f30a9f6c69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
